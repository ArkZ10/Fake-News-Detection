{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":6641062,"sourceType":"datasetVersion","datasetId":2093157}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"script","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-06-06T15:48:00.715544Z\",\"iopub.execute_input\":\"2024-06-06T15:48:00.716159Z\",\"iopub.status.idle\":\"2024-06-06T15:48:16.825537Z\",\"shell.execute_reply.started\":\"2024-06-06T15:48:00.716128Z\",\"shell.execute_reply\":\"2024-06-06T15:48:16.824591Z\"}}\n!pip install virtualenv\n!virtualenv mlops-tfx\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-06-06T15:48:16.827423Z\",\"iopub.execute_input\":\"2024-06-06T15:48:16.827704Z\",\"iopub.status.idle\":\"2024-06-06T15:48:17.820317Z\",\"shell.execute_reply.started\":\"2024-06-06T15:48:16.827679Z\",\"shell.execute_reply\":\"2024-06-06T15:48:17.818976Z\"}}\n!source mlops-tfx/bin/activate\n\n# %% [code] {\"scrolled\":true,\"execution\":{\"iopub.status.busy\":\"2024-06-06T15:48:17.821843Z\",\"iopub.execute_input\":\"2024-06-06T15:48:17.822171Z\",\"iopub.status.idle\":\"2024-06-06T17:16:21.118310Z\",\"shell.execute_reply.started\":\"2024-06-06T15:48:17.822140Z\",\"shell.execute_reply\":\"2024-06-06T17:16:21.117382Z\"}}\n!pip install jupyter scikit-learn tensorflow tfx==1.14.0 flask joblib\n\n# %% [markdown]\n# # Data Preprocessing\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-06-06T17:47:21.663984Z\",\"iopub.execute_input\":\"2024-06-06T17:47:21.664792Z\",\"iopub.status.idle\":\"2024-06-06T17:47:27.424501Z\",\"shell.execute_reply.started\":\"2024-06-06T17:47:21.664755Z\",\"shell.execute_reply\":\"2024-06-06T17:47:27.423495Z\"}}\nimport pandas as pd\nimport os\ndf = pd.read_csv(\"/kaggle/input/fake-news-classification/WELFake_Dataset.csv\")\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-06-06T17:47:27.426470Z\",\"iopub.execute_input\":\"2024-06-06T17:47:27.426852Z\",\"iopub.status.idle\":\"2024-06-06T17:47:27.437608Z\",\"shell.execute_reply.started\":\"2024-06-06T17:47:27.426818Z\",\"shell.execute_reply\":\"2024-06-06T17:47:27.436595Z\"}}\ndf = df[[\"text\", \"label\"]]\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-06-06T17:47:27.438696Z\",\"iopub.execute_input\":\"2024-06-06T17:47:27.439021Z\",\"iopub.status.idle\":\"2024-06-06T17:47:27.473695Z\",\"shell.execute_reply.started\":\"2024-06-06T17:47:27.438988Z\",\"shell.execute_reply\":\"2024-06-06T17:47:27.472844Z\"}}\ndf.isnull().any()\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-06-06T17:47:27.476006Z\",\"iopub.execute_input\":\"2024-06-06T17:47:27.476331Z\",\"iopub.status.idle\":\"2024-06-06T17:47:27.506482Z\",\"shell.execute_reply.started\":\"2024-06-06T17:47:27.476302Z\",\"shell.execute_reply\":\"2024-06-06T17:47:27.505582Z\"}}\ndf = df.dropna()\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-06-06T17:47:27.507641Z\",\"iopub.execute_input\":\"2024-06-06T17:47:27.507946Z\",\"iopub.status.idle\":\"2024-06-06T17:47:27.521205Z\",\"shell.execute_reply.started\":\"2024-06-06T17:47:27.507915Z\",\"shell.execute_reply\":\"2024-06-06T17:47:27.520312Z\"}}\ndf\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-06-06T17:47:27.522387Z\",\"iopub.execute_input\":\"2024-06-06T17:47:27.522723Z\",\"iopub.status.idle\":\"2024-06-06T17:47:27.535932Z\",\"shell.execute_reply.started\":\"2024-06-06T17:47:27.522694Z\",\"shell.execute_reply\":\"2024-06-06T17:47:27.535089Z\"}}\ndf = df.groupby('label', group_keys=False).apply(lambda x: x.sample(min(len(x), 200)))\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-06-06T17:47:27.536964Z\",\"iopub.execute_input\":\"2024-06-06T17:47:27.537223Z\",\"iopub.status.idle\":\"2024-06-06T17:47:27.544219Z\",\"shell.execute_reply.started\":\"2024-06-06T17:47:27.537201Z\",\"shell.execute_reply\":\"2024-06-06T17:47:27.543320Z\"}}\ndf['label'].value_counts()\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-06-06T17:47:27.545820Z\",\"iopub.execute_input\":\"2024-06-06T17:47:27.546233Z\",\"iopub.status.idle\":\"2024-06-06T17:47:27.558165Z\",\"shell.execute_reply.started\":\"2024-06-06T17:47:27.546202Z\",\"shell.execute_reply\":\"2024-06-06T17:47:27.557449Z\"}}\ndf.info()\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-06-06T17:47:27.559201Z\",\"iopub.execute_input\":\"2024-06-06T17:47:27.559474Z\",\"iopub.status.idle\":\"2024-06-06T17:47:27.570499Z\",\"shell.execute_reply.started\":\"2024-06-06T17:47:27.559450Z\",\"shell.execute_reply\":\"2024-06-06T17:47:27.569545Z\"}}\ndf.head(2)\n\n# %% [markdown]\n# # Apache Beam\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-06-06T17:47:42.590040Z\",\"iopub.execute_input\":\"2024-06-06T17:47:42.590872Z\",\"iopub.status.idle\":\"2024-06-06T17:47:42.594867Z\",\"shell.execute_reply.started\":\"2024-06-06T17:47:42.590839Z\",\"shell.execute_reply\":\"2024-06-06T17:47:42.593943Z\"}}\nTRANSFORM_MODULE_FILE = 'fake_news_detection_transform.py'\nTUNER_MODULE_FILE = 'fake_news_detection_tuner.py'\nTRAINER_MODULE_FILE = 'fake_news_detection_trainer.py'\nCOMPONENT_MODULE_FILE = 'fake_news_detection_component.py'\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-06-06T17:47:43.498548Z\",\"iopub.execute_input\":\"2024-06-06T17:47:43.499369Z\",\"iopub.status.idle\":\"2024-06-06T17:47:43.506145Z\",\"shell.execute_reply.started\":\"2024-06-06T17:47:43.499340Z\",\"shell.execute_reply\":\"2024-06-06T17:47:43.505274Z\"}}\n\n%%writefile {TRANSFORM_MODULE_FILE}\nimport tensorflow as tf\nimport os\nfrom nltk.corpus import stopwords\nimport nltk\n\nnltk.download('stopwords')\n\nLABEL_KEY   = \"label\"\nFEATURE_KEY = \"text\"\n\n# Define the set of stopwords\nstop_words = set(stopwords.words('english'))\n\n# Renaming transformed features\ndef transformed_name(key):\n    return key + \"_xf\"\n\n# Preprocess input features into transformed features\ndef preprocessing_fn(inputs):\n    \"\"\"\n    inputs:  map from feature keys to raw features\n    outputs: map from feature keys to transformed features\n    \"\"\"\n\n    outputs = {}\n\n    # Convert text to lowercase\n    text_lower = tf.strings.lower(inputs[FEATURE_KEY])\n\n    # Remove stopwords\n    text_without_stopwords = tf.strings.regex_replace(text_lower, '|'.join(stop_words), '')\n\n    # Store transformed text feature\n    outputs[transformed_name(FEATURE_KEY)] = text_without_stopwords\n\n    outputs[transformed_name(LABEL_KEY)]   = tf.cast(inputs[LABEL_KEY], tf.int64)\n\n\n    return outputs\n\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-06-06T17:47:44.162639Z\",\"iopub.execute_input\":\"2024-06-06T17:47:44.162973Z\",\"iopub.status.idle\":\"2024-06-06T17:47:44.170886Z\",\"shell.execute_reply.started\":\"2024-06-06T17:47:44.162944Z\",\"shell.execute_reply\":\"2024-06-06T17:47:44.169914Z\"}}\n\n\n%%writefile {TUNER_MODULE_FILE}\nimport os\nimport tensorflow as tf\nimport tensorflow_transform as tft\nimport keras_tuner as kt\nfrom tensorflow.keras import layers\nfrom tfx.components.trainer.fn_args_utils import FnArgs\nfrom keras_tuner.engine import base_tuner\nfrom typing import NamedTuple, Dict, Text, Any\n\nLABEL_KEY   = 'label'\nFEATURE_KEY = 'text'\n\ndef transformed_name(key):\n    \"\"\"Renaming transformed features\"\"\"\n    return key + \"_xf\"\n\ndef gzip_reader_fn(filenames):\n    \"\"\"Loads compressed data\"\"\"\n    return tf.data.TFRecordDataset(filenames, compression_type='GZIP')\n\ndef input_fn(file_pattern, tf_transform_output, num_epochs, batch_size=128) -> tf.data.Dataset:\n    \"\"\"Get post_tranform feature & create batches of data\"\"\"\n    \n    transform_feature_spec = (\n        tf_transform_output.transformed_feature_spec().copy()\n    )\n    \n    # create batches of data\n    dataset = tf.data.experimental.make_batched_features_dataset(\n        file_pattern = file_pattern,\n        batch_size   = batch_size,\n        features     = transform_feature_spec,\n        reader       = gzip_reader_fn,\n        num_epochs   = num_epochs,\n        label_key    = transformed_name(LABEL_KEY)\n    )\n\n    return dataset\n\n# Vocabulary size and number of words in a sequence.\nVOCAB_SIZE      = 1000\nSEQUENCE_LENGTH = 500\n\nvectorize_layer = layers.TextVectorization(\n    standardize            = 'lower_and_strip_punctuation',\n    max_tokens             = VOCAB_SIZE,\n    output_mode            = 'int',\n    output_sequence_length = SEQUENCE_LENGTH\n)\n\ndef model_builder(hp):\n    \"\"\"Build keras tuner model\"\"\"\n    embedding_dim = hp.Int('embedding_dim', min_value=30, max_value=40, step=5)\n    lstm_units    = hp.Int('lstm_units', min_value=32, max_value=96, step=16)\n    dropout_rate = hp.Float('dropout_rate', min_value=0.3, max_value=0.5, step=0.1)\n    learning_rate = hp.Choice('learning_rate', values=[1e-2])\n    \n    inputs = tf.keras.Input(shape=(1,), name=transformed_name(FEATURE_KEY), dtype=tf.string)\n    \n    reshaped_narrative = tf.reshape(inputs, [-1])\n    x = vectorize_layer(reshaped_narrative)\n    x = layers.Embedding(VOCAB_SIZE, embedding_dim, name='embedding')(x)\n    x = layers.Dropout(dropout_rate)(x)\n    x = layers.Bidirectional(layers.LSTM(lstm_units, return_sequences=True))(x)\n    x = layers.Bidirectional(layers.LSTM(lstm_units))(x)\n    x = layers.Dropout(dropout_rate)(x)\n    x = layers.BatchNormalization()(x)\n    outputs = layers.Dense(2, activation='softmax')(x)\n    \n    model = tf.keras.Model(inputs = inputs, outputs = outputs)\n    model.compile(\n        loss      = tf.keras.losses.SparseCategoricalCrossentropy(),\n        optimizer = tf.keras.optimizers.Adam(learning_rate),\n        metrics   = ['accuracy']\n    )\n    \n    model.summary()\n    return model\n\nTunerFnResult = NamedTuple('TunerFnResult', [\n    ('tuner', base_tuner.BaseTuner),\n    ('fit_kwargs', Dict[Text, Any]),\n])\n\nearly_stop_callback = tf.keras.callbacks.EarlyStopping(\n    monitor  = 'val_accuracy',\n    mode     = 'max',\n    verbose  = 1,\n    patience = 2\n)\n\ndef tuner_fn(fn_args: FnArgs) -> None:\n    # Load the transform output\n    tf_transform_output = tft.TFTransformOutput(fn_args.transform_graph_path)\n    \n    # Create batches of data\n    train_set = input_fn(fn_args.train_files[0], tf_transform_output, 5)\n    val_set   = input_fn(fn_args.eval_files[0],  tf_transform_output, 5)\n\n    vectorize_layer.adapt(\n        [j[0].numpy()[0] for j in [\n            i[0][transformed_name(FEATURE_KEY)]\n                for i in list(train_set)\n        ]]\n    )\n    \n    # Build the model tuner\n    model_tuner = kt.Hyperband(\n        hypermodel   = lambda hp: model_builder(hp),\n        objective    = kt.Objective('val_accuracy', direction='max'),\n        max_epochs   = 5,\n        factor       = 2,\n        directory    = fn_args.working_dir,\n        project_name = 'fake_news_detection'\n    )\n    model_tuner.oracle.max_trials = 10\n\n    return TunerFnResult(\n        tuner      = model_tuner,\n        fit_kwargs = {\n            'callbacks'        : [early_stop_callback],\n            'x'                : train_set,\n            'validation_data'  : val_set,\n            'steps_per_epoch'  : fn_args.train_steps,\n            'validation_steps' : fn_args.eval_steps\n        }\n    )\n     \n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-06-06T17:47:44.825618Z\",\"iopub.execute_input\":\"2024-06-06T17:47:44.825948Z\",\"iopub.status.idle\":\"2024-06-06T17:47:44.834799Z\",\"shell.execute_reply.started\":\"2024-06-06T17:47:44.825922Z\",\"shell.execute_reply\":\"2024-06-06T17:47:44.833890Z\"}}\n\n\n%%writefile {TRAINER_MODULE_FILE}\nimport os\nimport tensorflow as tf\nimport tensorflow_transform as tft\nfrom tensorflow.keras import layers\nfrom tfx.components.trainer.fn_args_utils import FnArgs\n\nLABEL_KEY   = 'label'\nFEATURE_KEY = 'text'\n\ndef transformed_name(key):\n    \"\"\"Renaming transformed features\"\"\"\n    return key + \"_xf\"\n\ndef gzip_reader_fn(filenames):\n    \"\"\"Loads compressed data\"\"\"\n    return tf.data.TFRecordDataset(filenames, compression_type='GZIP')\n\ndef input_fn(file_pattern, tf_transform_output, num_epochs, batch_size=128) -> tf.data.Dataset:\n    \"\"\"Get post_tranform feature & create batches of data\"\"\"\n    \n    transform_feature_spec = (\n        tf_transform_output.transformed_feature_spec().copy()\n    )\n    \n    # create batches of data\n    dataset = tf.data.experimental.make_batched_features_dataset(\n        file_pattern = file_pattern,\n        batch_size   = batch_size,\n        features     = transform_feature_spec,\n        reader       = gzip_reader_fn,\n        num_epochs   = num_epochs,\n        label_key    = transformed_name(LABEL_KEY)\n    )\n\n    return dataset\n\n# Vocabulary size and number of words in a sequence\nVOCAB_SIZE      = 1000\nSEQUENCE_LENGTH = 500\n# embedding_dim   = 35\n\nvectorize_layer = layers.TextVectorization(\n    standardize            = 'lower_and_strip_punctuation',\n    max_tokens             = VOCAB_SIZE,\n    output_mode            = 'int',\n    output_sequence_length = SEQUENCE_LENGTH\n)\n\ndef model_builder(hp):\n    \"\"\"Build keras tuner model\"\"\"\n    inputs = tf.keras.Input(shape=(1,), name=transformed_name(FEATURE_KEY), dtype=tf.string)\n    \n    reshaped_narrative = tf.reshape(inputs, [-1])\n    x = vectorize_layer(reshaped_narrative)\n    x = layers.Embedding(VOCAB_SIZE, hp['embedding_dim'], name='embedding')(x)\n    x = layers.Dropout(hp['dropout_rate'])(x)\n    x = layers.Bidirectional(layers.LSTM(hp['lstm_units'], return_sequences=True))(x)\n    x = layers.Bidirectional(layers.LSTM(hp['lstm_units']))(x)\n    x = layers.Dropout(hp['dropout_rate'])(x)\n    x = layers.BatchNormalization()(x)\n    outputs = layers.Dense(2, activation='softmax')(x)\n    \n    model = tf.keras.Model(inputs = inputs, outputs = outputs)\n    model.compile(\n        loss      = tf.keras.losses.SparseCategoricalCrossentropy(),\n        optimizer = tf.keras.optimizers.Adam(hp['learning_rate']),\n        metrics   = ['accuracy']\n    )\n    \n    model.summary()\n    return model\n\ndef _get_serve_tf_examples_fn(model, tf_transform_output):\n    model.tft_layer = tf_transform_output.transform_features_layer()\n    \n    @tf.function\n    def serve_tf_examples_fn(serialized_tf_examples):\n        feature_spec = tf_transform_output.raw_feature_spec()\n        feature_spec.pop(LABEL_KEY)\n\n        parsed_features      = tf.io.parse_example(serialized_tf_examples, feature_spec)\n        transformed_features = model.tft_layer(parsed_features)\n        \n        # get predictions using the transformed features\n        return model(transformed_features)\n        \n    return serve_tf_examples_fn\n    \n    \nclass AccuracyThresholdCallback(tf.keras.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs=None):\n        if logs.get('accuracy') >= 0.80 and logs.get('val_accuracy') >= 0.80:\n            print(f\"\\nEpoch {epoch}: Training and validation accuracy have both reached 80%. Stopping training.\")\n            self.model.stop_training = True\n    \ndef run_fn(fn_args: FnArgs) -> None:\n    log_dir = os.path.join(os.path.dirname(fn_args.serving_model_dir), 'logs')\n    hp      = fn_args.hyperparameters['values']\n    hp['epochs'] = 20\n    \n    \n    tensorboard_callback = tf.keras.callbacks.TensorBoard(\n        log_dir = log_dir, update_freq='batch'\n    )\n    \n    early_stop_callback = tf.keras.callbacks.EarlyStopping(\n        monitor  = 'val_binary_accuracy',\n        mode     = 'max',\n        verbose  = 1,\n        patience = 10\n    )\n\n    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n        fn_args.serving_model_dir,\n        monitor        = 'val_binary_accuracy',\n        mode           = 'max',\n        verbose        = 1,\n        save_best_only = True\n    )\n    \n    accuracy_threshold_callback = AccuracyThresholdCallback()\n\n    callbacks = [\n        tensorboard_callback,\n        early_stop_callback,\n        model_checkpoint_callback,\n        accuracy_threshold_callback\n    ]\n    \n    # Load the transform output\n    tf_transform_output = tft.TFTransformOutput(fn_args.transform_graph_path)\n    \n    # Create batches of data\n    train_set = input_fn(fn_args.train_files, tf_transform_output, hp['tuner/epochs'])\n    val_set   = input_fn(fn_args.eval_files,  tf_transform_output, hp['tuner/epochs'])\n\n    vectorize_layer.adapt(\n        [j[0].numpy()[0] for j in [\n            i[0][transformed_name(FEATURE_KEY)]\n                for i in list(train_set)\n        ]]\n    )\n    \n    # Build the model\n    model = model_builder(hp)\n    \n    # Train the model\n    model.fit(\n        x                = train_set,\n        validation_data  = val_set,\n        callbacks        = callbacks,\n        steps_per_epoch  = fn_args.train_steps,\n        validation_steps = fn_args.eval_steps,\n        epochs           = hp['epochs']\n    )\n\n    signatures = {\n        'serving_default': _get_serve_tf_examples_fn(\n            model, tf_transform_output\n        ).get_concrete_function(\n            tf.TensorSpec(\n                shape = [None],\n                dtype = tf.string,\n                name  = 'examples'\n            )\n        )\n    }\n\n    model.save(\n        fn_args.serving_model_dir,\n        save_format = 'tf',\n        signatures  = signatures\n    )\n     \n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-06-06T17:47:45.442159Z\",\"iopub.execute_input\":\"2024-06-06T17:47:45.442940Z\",\"iopub.status.idle\":\"2024-06-06T17:47:45.451803Z\",\"shell.execute_reply.started\":\"2024-06-06T17:47:45.442898Z\",\"shell.execute_reply\":\"2024-06-06T17:47:45.450848Z\"}}\n\n\n%%writefile {COMPONENT_MODULE_FILE}\n\nimport os\nimport tensorflow_model_analysis as tfma\nfrom tfx.components import (\n    CsvExampleGen,\n    StatisticsGen,\n    SchemaGen,\n    ExampleValidator,\n    Transform,\n    Tuner,\n    Trainer,\n    Evaluator,\n    Pusher\n)\nfrom tfx.dsl.components.common.resolver import Resolver\nfrom tfx.dsl.input_resolution.strategies.latest_blessed_model_strategy import (\n    LatestBlessedModelStrategy)\nfrom tfx.proto import example_gen_pb2, trainer_pb2, pusher_pb2\nfrom tfx.types import Channel\nfrom tfx.types.standard_artifacts import Model, ModelBlessing\n\n\ndef init_components(args):\n    \"\"\"Initiate tfx pipeline components\n\n    Args:\n        args (dict) :\n            data_dir (str): a path to the data\n            transform_module (str): a path to the transform_module\n            tuner_module (str): a path to the tuner_module\n            training_module (str): a path to the transform_module\n            training_steps (int): number of training steps\n            eval_steps (int): number of eval steps\n            serving_model_dir (str): a path to the serving model directory\n\n    Returns:\n        tuple: TFX components\n    \"\"\"\n    output = example_gen_pb2.Output(\n        split_config=example_gen_pb2.SplitConfig(splits=[\n            example_gen_pb2.SplitConfig.Split(name='train', hash_buckets=8),\n            example_gen_pb2.SplitConfig.Split(name='eval', hash_buckets=2)\n        ])\n    )\n    \n\n    example_gen = CsvExampleGen(\n        input_base=args['Data'],\n        output_config=output\n    )\n\n    statistics_gen = StatisticsGen(\n        examples=example_gen.outputs['examples']\n    )\n\n    schema_gen = SchemaGen(\n        statistics=statistics_gen.outputs['statistics']\n    )\n\n    example_validator = ExampleValidator(\n        statistics=statistics_gen.outputs['statistics'],\n        schema=schema_gen.outputs['schema']\n    )\n\n    transform  = Transform(\n        examples=example_gen.outputs['examples'],\n        schema=schema_gen.outputs['schema'],\n        module_file=os.path.abspath(args['transform_module'])\n    )\n    \n\n    tuner = Tuner(\n        module_file=os.path.abspath(args['tuner_module']),\n        examples=transform.outputs['transformed_examples'],\n        transform_graph=transform.outputs['transform_graph'],\n        schema=schema_gen.outputs['schema'],\n        train_args      = trainer_pb2.TrainArgs(splits=['train']),\n        eval_args       = trainer_pb2.EvalArgs(splits=['eval'])\n    )\n\n    trainer = Trainer(\n        module_file=os.path.abspath(args['trainer_module']),\n        examples=transform.outputs['transformed_examples'],\n        transform_graph=transform.outputs['transform_graph'],\n        schema=schema_gen.outputs['schema'],\n        hyperparameters=tuner.outputs['best_hyperparameters'],\n        train_args      = trainer_pb2.TrainArgs(splits=['train']),\n        eval_args       = trainer_pb2.EvalArgs(splits=['eval'])\n    )\n    \n\n    model_resolver = Resolver(\n        strategy_class=LatestBlessedModelStrategy,\n        model=Channel(type=Model),\n        model_blessing=Channel(type=ModelBlessing)\n    ).with_id('Latest_blessed_model_resolver')\n\n#     slicing_specs=[\n#         tfma.SlicingSpec(),\n#         tfma.SlicingSpec(feature_keys=[\n#             'gender',\n#             'ever_married'\n#         ])\n#     ]\n\n    metrics_specs = [\n        tfma.MetricsSpec(metrics=[\n            tfma.MetricConfig(class_name = 'ExampleCount'),\n            tfma.MetricConfig(class_name = 'AUC'),\n            tfma.MetricConfig(class_name = 'FalsePositives'),\n            tfma.MetricConfig(class_name = 'TruePositives'),\n            tfma.MetricConfig(class_name = 'FalseNegatives'),\n            tfma.MetricConfig(class_name = 'TrueNegatives'),\n            tfma.MetricConfig(class_name = 'BinaryAccuracy',\n                threshold=tfma.MetricThreshold(\n                    value_threshold = tfma.GenericValueThreshold(\n                        lower_bound = {'value': 0.5}\n                    ),\n                    change_threshold = tfma.GenericChangeThreshold(\n                        direction = tfma.MetricDirection.HIGHER_IS_BETTER,\n                        absolute  = {'value': 0.0001}\n                    )\n                )\n            )\n        ])\n    ]\n    \n\n    eval_config = tfma.EvalConfig(\n        model_specs=[tfma.ModelSpec(label_key='label')],\n        slicing_specs=[tfma.SlicingSpec()],\n        metrics_specs=metrics_specs\n    )\n\n    evaluator = Evaluator(\n        examples=example_gen.outputs['examples'],\n        model=trainer.outputs['model'],\n        baseline_model=model_resolver.outputs['model'],\n        eval_config=eval_config\n    )\n\n    pusher = Pusher(\n        model=trainer.outputs['model'],\n        model_blessing=evaluator.outputs['blessing'],\n        push_destination=pusher_pb2.PushDestination(\n            filesystem=pusher_pb2.PushDestination.Filesystem(\n                base_directory=args['serving_model_dir/fake_news_detection']\n            )\n        ),\n    )\n    \n\n    return (\n        example_gen,\n        statistics_gen,\n        schema_gen,\n        example_validator,\n        transform,\n        tuner,\n        trainer,\n        model_resolver,\n        evaluator,\n        pusher\n    )\n\n# %% [markdown]\n# # PIPELINE INITIALIZATION\n\n# %% [code]\nshutil.rmtree(destination_folder)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-06-06T17:47:51.437282Z\",\"iopub.execute_input\":\"2024-06-06T17:47:51.437892Z\",\"iopub.status.idle\":\"2024-06-06T17:47:51.443625Z\",\"shell.execute_reply.started\":\"2024-06-06T17:47:51.437861Z\",\"shell.execute_reply\":\"2024-06-06T17:47:51.442626Z\"}}\nimport os\nimport shutil\n\n# Define the destination folder\ndestination_folder = '/kaggle/working/modules'  # Replace 'modules' with your desired folder name\n\n# Create the destination folder if it doesn't exist\nos.makedirs(destination_folder)\n\n# Define the files you want to move\nfiles_to_move = [\n    '/kaggle/working/fake_news_detection_tuner.py',\n    '/kaggle/working/fake_news_detection_component.py',\n    '/kaggle/working/fake_news_detection_trainer.py',\n    '/kaggle/working/fake_news_detection_transform.py'\n]\n\n# Move each file to the new folder\nfor file_to_move in files_to_move:\n    shutil.move(file_to_move, destination_folder)\n\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-06-06T17:49:03.574155Z\",\"iopub.execute_input\":\"2024-06-06T17:49:03.575019Z\",\"iopub.status.idle\":\"2024-06-06T17:49:28.305477Z\",\"shell.execute_reply.started\":\"2024-06-06T17:49:03.574986Z\",\"shell.execute_reply\":\"2024-06-06T17:49:28.304720Z\"}}\nimport os\nimport pandas as pd\nfrom typing import Text\n\nfrom absl import logging\nfrom tfx.orchestration import metadata, pipeline\nfrom tfx.orchestration.beam.beam_dag_runner import BeamDagRunner\nfrom modules import fake_news_detection_component\n\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-06-06T17:50:16.175390Z\",\"iopub.execute_input\":\"2024-06-06T17:50:16.176128Z\",\"iopub.status.idle\":\"2024-06-06T17:50:16.181342Z\",\"shell.execute_reply.started\":\"2024-06-06T17:50:16.176094Z\",\"shell.execute_reply\":\"2024-06-06T17:50:16.180443Z\"}}\nPIPELINE_NAME = \"fake-news-detection\"\n\n# Pipeline inputs\nDATA_ROOT = 'Data'\nTRANSFORM_MODULE_FILE = 'modules/fake_news_detection_transform.py'\nTUNER_MODULE_FILE = 'modules/fake_news_detection_tuner.py'\nTRAINER_MODULE_FILE = 'modules/fake_news_detection_trainer.py'\n\n# Pipeline outputs\nOUTPUT_BASE = 'outputs'\n\nserving_model_dir = os.path.join(OUTPUT_BASE, 'serving_model')\npipeline_root = os.path.join(OUTPUT_BASE, PIPELINE_NAME)\nmetadata_path = os.path.join(pipeline_root, 'metadata.sqlite')\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-06-06T17:53:52.600357Z\",\"iopub.execute_input\":\"2024-06-06T17:53:52.600745Z\",\"iopub.status.idle\":\"2024-06-06T18:05:10.766322Z\",\"shell.execute_reply.started\":\"2024-06-06T17:53:52.600716Z\",\"shell.execute_reply\":\"2024-06-06T18:05:10.765362Z\"}}\ndef init_local_pipeline(\n    fake_news_detection_component, pipeline_root: Text\n) -> pipeline.Pipeline:\n    \"\"\"Init local pipeline\n\n    Args:\n        components (dict): tfx components\n        pipeline_root (Text): path to pipeline directory\n\n    Returns:\n        pipeline.Pipeline: apache beam pipeline orchestration\n    \"\"\"\n    logging.info(f\"Pipeline root set to: {pipeline_root}\")\n    beam_args = [\n        '--direct_running_mode=multi_processing'\n        # 0 auto-detect based on on the number of CPUs available\n        # during execution time.\n        '----direct_num_workers=0'\n    ]\n\n    return pipeline.Pipeline(\n        pipeline_name=PIPELINE_NAME,\n        pipeline_root=pipeline_root,\n        components=components,\n        enable_cache=True,\n        metadata_connection_config=metadata.sqlite_metadata_connection_config(\n            metadata_path\n        ),\n        eam_pipeline_args=beam_args\n    )\n     \n\nlogging.set_verbosity(logging.INFO)\n\ncomponents = fake_news_detection_component.init_components({\n    'Data': DATA_ROOT,\n    'transform_module': TRANSFORM_MODULE_FILE,\n    'tuner_module': TUNER_MODULE_FILE,\n    'trainer_module': TRAINER_MODULE_FILE,\n    'training_steps': 5000,\n    'eval_steps': 1000,\n    'serving_model_dir/fake_news_detection': serving_model_dir\n})\n\npipeline = init_local_pipeline(components, pipeline_root)\nBeamDagRunner().run(pipeline=pipeline)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-06-06T18:13:18.137446Z\",\"iopub.execute_input\":\"2024-06-06T18:13:18.137840Z\",\"iopub.status.idle\":\"2024-06-06T18:13:26.757604Z\",\"shell.execute_reply.started\":\"2024-06-06T18:13:18.137809Z\",\"shell.execute_reply\":\"2024-06-06T18:13:26.756271Z\"},\"scrolled\":true}\n!zip -r Project2.zip /kaggle/working/\n!pip freeze > requirements.txt\n\n# %% [code]\n","metadata":{"_uuid":"50eb4d1b-60b0-4c56-b54f-9a5f1514b5ed","_cell_guid":"58369996-d97f-41c9-880b-2f02f5b90292","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}